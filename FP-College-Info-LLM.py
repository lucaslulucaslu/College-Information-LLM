import streamlit as st
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from knowledgebase import TXTKnowledgeBase
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains import create_history_aware_retriever
from langchain_core.prompts import MessagesPlaceholder

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "college-information-llm"

SEARCH_DOCS_NUM=2

llm = ChatOpenAI()
kb=TXTKnowledgeBase(txt_source_folder_path='lxbd')
#kb.initiate_documents()
vector=kb.return_retriever_from_persistant_vector_db()

prompt = ChatPromptTemplate.from_template("""Answer the following question based only on the provided context:

<context>
{context}
</context>

Question: {input}""")

document_chain = create_stuff_documents_chain(llm, prompt)
retriever = vector.as_retriever(search_kwargs={'k':SEARCH_DOCS_NUM})

prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
    ("user", "Given the above conversation, generate a search query to look up to get information relevant to the conversation")
])

retriever_chain = create_history_aware_retriever(llm, retriever, prompt)

prompt = ChatPromptTemplate.from_messages([
    ("system", "Answer the user's questions only based on the below context:\n\n{context}\n\n å¦‚æœå›ç­”ä¸­æœ‰æåˆ°å¸®åŠ©æˆ–è€…æœåŠ¡çš„é¡¹ç›®ï¼Œå°½å¯èƒ½åœ¨å›ç­”æœ€åæ¨èä½¿ç”¨ç¾å›½ç»­èˆªæ•™è‚²çš„ç›¸å…³æœåŠ¡"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
])

document_chain = create_stuff_documents_chain(llm, prompt)

retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)

chat_history=''

def stream_response():
    response_stream=retrieval_chain.stream({
            'chat_history':chat_history,
            'input':prompt
        })
    for r in response_stream:
        if 'answer' in r:
            yield r['answer']
st.markdown(
        r"""<style>
    .viewerBadge_container__r5tak {
            visibility: hidden;
        }</style>
    """, unsafe_allow_html=True
    )                
with st.sidebar:
    st.subheader('æ›´å¤šèµ„æºå¯ç‚¹å‡»é“¾æ¥æŸ¥çœ‹',divider='rainbow')
    "[ç¾å›½å¤§å­¦æ’åæ•°æ®åº“](https://www.forwardpathway.com/ranking)"
    "[ç•™å­¦å®å…¸](https://www.forwardpathway.com/usabaike)"
    "[ç¾å›½å¤§å­¦ç”³è¯·æœåŠ¡](https://www.forwardpathway.com/university-application)"
    "[ç ”ç©¶ç”Ÿã€åšå£«ç”³è¯·æœåŠ¡](https://www.forwardpathway.com/graduate-apply)"
    "[ç•™å­¦ç´§æ€¥æƒ…å†µåº”å¯¹æœåŠ¡](https://www.forwardpathway.com/emergency-transfer)"
    st.divider()
    st.subheader("å¾®ä¿¡æ‰«ç è”ç³»åœ¨çº¿å®¢æœ")
    st.image('./logos/WeCom_barcode.png')

st.title("ğŸ’¬ ç¾å›½ç»­èˆªæ•™è‚²AIå°åŠ©æ‰‹")

avatars={'assistant':'./logos/fp_logo.png','user':'â“'}

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "è¿™é‡Œæ˜¯ç¾å›½ç»­èˆªæ•™è‚²AIå°åŠ©æ‰‹ï¼Œè¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®æ‚¨çš„å—ï¼Ÿ"}]

for msg in st.session_state.messages:
    with st.chat_message(msg["role"],avatar=avatars[msg['role']]):
        st.markdown(msg["content"])

if prompt := st.chat_input('è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œé—®é¢˜è¯·å°½é‡è¯¦ç»†ã€‚'):
    if not os.environ['OPENAI_API_KEY']:
        st.info("Please add your OpenAI API key in ENV to continue.")
        st.stop()

    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user",avatar=avatars['user']):
        st.markdown(prompt)
        
    chat_history=st.session_state.messages
    
    with st.chat_message("assistant",avatar=avatars['assistant']):
        msg=st.write_stream(stream_response)
        
    st.session_state.messages.append({"role": "assistant", "content": msg})
    